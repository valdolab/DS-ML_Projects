{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valdolab/DS_mAIstros/blob/main/Notes/Week3_Theory_Regularization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaUKntU7UI4W"
      },
      "source": [
        "Taylor Series Aproximation for Model Regularization\n",
        "\n",
        "\n",
        "the bias of your model: $B[\\hat \\theta] \\equiv\\mathbb{E}[\\hat \\theta] - \\theta$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7kTFXqwUI4c"
      },
      "source": [
        "Regularization:\n",
        "    \n",
        "  Take our objective funciton:\n",
        "    \n",
        "$\\mathcal J=\\sum_{i=1}^N (y_i-\\hat y_i)^2+ \\frac{\\lambda}{2} \\sum_{j=0}^D w_j^2$\n",
        "\n",
        "$= \\frac{1}{2N} \\sum_{i=1}^N (y_1-\\hat y_i)^2 +\\frac{\\lambda}{2N} \\sum_{j=0}^D w_j^2$  but the last term is the norm of w, so\n",
        "\n",
        "$\\mathcal J=\\frac{1}{2N}( (y_i-\\hat y_i)^T  (y_i-\\hat y_i)  + \\lambda w^T w)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhkdN52XUI4d"
      },
      "source": [
        "$\\mathcal J=\\frac{1}{2N}( (y^T Y -2 y^t \\hat y + \\hat y^t \\hat y + \\lambda w^T w)$\n",
        "\n",
        "$=\\frac{1}{2N} ( y^T y 2 y^T \\Phi w + w^T \\Phi^T \\Phi w +\\lambda w^T w)$\n",
        "\n",
        "Now differentiate and set equal to 0:\n",
        "\n",
        "$\\nabla \\mathcal J =\\frac{1}{N} (-y^T \\Phi +w^T |Phi^T \\Phi +\\lambda w^T)=0$\n",
        "\n",
        "$\\Phi^T \\Phi w +\\lambda w = \\Phi^T y \\Rightarrow (\\Phi^T \\Phi + \\lambda I) w = \\Phi^T y$\n",
        "\n",
        "$w=(\\Phi^T \\Phi+\\lambda I)^{-1} \\Phi^T y$   where I is the identity matrix and $\\lambda$ is the Regularization Constant\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLrcqwCeUI4d"
      },
      "source": [
        "Updated Gradient Descent  Update rule:\n",
        "\n",
        "$\\nabla \\mathcal J = \\frac{1}{N} [ \\Phi^T (\\hat y - y ) +\\lambda w]$   we can see that if $\\lambda$ is zero we get our old update rule.\n",
        "\n",
        "\n",
        "$w_{t+1} = w_t -\\eta \\frac{1}{N} [ \\Phi^T (\\hat y - y ) +\\lambda w-t] $\n",
        "\n",
        "$w_{t+1} = w_t - \\eta \\frac{\\lambda}{N} w_t -\\eta \\frac{1}{N} \\Phi^T (\\hat y - y)$\n",
        "\n",
        "$w_{t+1} =( 1-\\eta \\frac{\\lambda}{N}) w_t - \\eta \\frac{1}{N} \\Phi^T (\\hat y -y )$  where $( 1-\\eta \\frac{\\lambda}{N}) w_t$ is a correction to $w_{t+1}$ thus moderating th wheights with each iteration.\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3bZYKODUI4e"
      },
      "source": [
        "Probabilistic view:\n",
        "\n",
        "let $\\mathcal{l} = \\mathcal{J}$  and $\\mathcal L = e^{\\mathcal{l}}$  and thus, $\\mathcal{L} = e^{-\\mathcal J}$\n",
        "\n",
        "$\\mathcal{L} = e^{-\\sum_{i=1}^N (y_i-\\hat y_i)^2 + \\lambda \\sum_{j=1}^{P} w_j^2)}$\n",
        "\n",
        "$\\mathcal{L} = \\prod_{i=1}^{N} e^{-(y_i-\\hat y_i)^2}  \\prod_{j=1}^P e^{-\\lambda w_j^2}$\n",
        "\n",
        "$\\mathcal L \\propto p(y \\vert X,w ) p(w)$\n",
        "\n",
        "$\\mathcal L \\propto p(w \\vert X,y)$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ld7fRk9UI4e"
      },
      "source": [
        "$w_{t+1} =w_t - \\eta \\frac{1}{N} \\Phi^T (\\hat y -y )$  where $( 1-\\eta \\frac{\\lambda}{N}) w_t$ is a correction to $w_{t+1}$ thus moderating th wheights with each iteration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzXodrLjUI4f"
      },
      "source": [
        "Practical Consdertions:\n",
        "\n",
        "$w_{t+1} = w_t -\\eta \\frac{1}{N} [ \\Phi^T (\\hat y - y ) +\\lambda w_t] $\n",
        "\n",
        "So, $(w_j)_{t+j}= (w_t)_t -\\eta \\frac{1}{N}  \\phi_j^T (\\hat y-y)$  therefore to separate b:\n",
        "\n",
        "$b_{t+1} = b_t -\\eta \\frac{1}{N}(\\hat y -y)$\n",
        "\n",
        "Conversely, if $w\\in \\mathbb{R}^{P+1}$\n",
        "\n",
        "$w=\\mat\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqMmuX7NUI4f"
      },
      "source": [
        "    \n",
        "   Regularization continued:  L1 Regularization\n",
        "   \n",
        "   \"LASSO\" Regression: Least Absolute Shrinkage & Selection Operator\n",
        "   \n",
        "   Let $l=-\\mathcal{-J}$  and $\\mathcal L = e^{l}$\n",
        "   \n",
        "   Then,\n",
        "   \n",
        " Laplacian:\n",
        " \n",
        " $p(x)=\\frac{1}{2b} e^{-\\frac{\\vert x-\\mu \\vert}{b}}$\n",
        " \n",
        " where \"diversity\" $\\sigma$\n",
        " \n",
        " such that $\\sigma ^2 = 2b^2$   and  $b=\\frac{\\sigma}{\\sqrt 2}$\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cW-jnEZiUI4g"
      },
      "source": [
        "$\\mathcal{L} = e^{-\\sum_{i=1}^N (y_i-\\hat y_i)^2 - \\lambda \\sum_{j=1}^{P} w_j^2)}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-a224kY6UI4g"
      },
      "source": [
        "$\\mathcal{L} = \\prod_{i=1}^{N} e^{-(y_i-\\hat y_i)^2}  \\prod_{j=1}^P e^{-\\lambda\\vert w_j \\vert}$\n",
        "\n",
        "$\\mathcal L \\propto p(y \\vert X,w ) p(w)$\n",
        "\n",
        "$\\mathcal L \\propto p(w \\vert X,y)$\n",
        "\n",
        "\n",
        "which leads to  Objective function:\n",
        "\n",
        "$$\\mathcal J = \\frac{1}{2N} \\sum_{i=1}^N (y_i-\\hat y_i)^2 + \\frac {\\lambda}{N} \\sum_{j=1}^{P} \\vert w_j \\vert$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiIUI5KlUI4h"
      },
      "source": [
        "In order to deal with the undefined absolute value function at $w_j=0$,\n",
        "\n",
        "We take $\\frac{d}{d w_j} \\vert w_j\\vert = sign(w_j)$\n",
        "\n",
        "so $\\nabla \\mathcal J $ becomes:\n",
        "\n",
        "$\\nabla \\mathcal J = \\frac{1}{N} [ \\frac {1}{2} (-2y^T \\Phi +2w^t \\Phi^T \\Phi)+ \\lambda$ sign$(w^T)]$\n",
        "\n",
        "$\\nabla \\mathcal J = \\frac{1}{N}   (-y^T \\Phi +w^t \\Phi^T \\Phi+ \\lambda$ sign$(w^T))$\n",
        "\n",
        "$$\\nabla \\mathcal J = \\frac{1}{N}(\\Phi^T (\\hat y - y ) +\\lambda \\mathbb sign(w))$$\n",
        "\n",
        "\n",
        "\n",
        "Elastic Net Regresion: Add both L1 and L2\n",
        "\n",
        "$$\\mathcal J=\\sum_{i=1}^N (y_i-\\hat y_i)^2+ \\frac{\\lambda_2}{2} \\sum_{j=0}^D w_j^2+\\frac {\\lambda_1}{N} \\sum_{j=1}^{P} \\vert w_j \\vert$$     This we can code up\n",
        "\n",
        "LP Regularization- a generalized Regularization\n",
        "\n",
        "$$\\nabla \\mathcal J = \\frac{1}{N}[\\Phi^T(\\hat y - y )+\\lambda_2 w +\\lambda_1 \\mathbb sign(w)]$$  and  $$\\hat y=\\Phi w$$\n",
        "\n",
        "\n",
        "Thus ,  $$\\vert \\vert w\\vert \\vert _p \\equiv (\\sum_{j=1}^P \\vert w_j\\vert^P)^{\\frac{2}{p}}$$\n",
        "\n",
        "\n",
        "Our Objective funtion now becomes:\n",
        "\n",
        "$$\\mathcal J = \\frac{1}{2N} \\sum_{i=1}^N (\\hat y -y)^2 +\\frac{\\lambda}{pN} \\sum_{j=1}^{P} \\vert w_j \\vert ^P$$\n",
        "\n",
        "\n",
        "And the Gradient becomes:\n",
        "\n",
        "$$\\nabla \\mathcal J =\\frac{1}{N}[\\Phi^T (\\hat y-y) +\\lambda \\vert w\\vert ^{p-1} \\odot  \\mathbf{sign} (w)]$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeEPuA-qUI4h"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tg6i-bf3UI4i"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
